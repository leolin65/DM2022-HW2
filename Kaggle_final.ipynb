{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: leo lin\n",
    "\n",
    "Student ID: 109003814\n",
    "\n",
    "GitHub ID: leolin65\n",
    "\n",
    "Kaggle name: leolin65\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "1. data_identification: define which twitter ID are train and which one are test data(csv)\n",
    "2. emotion: Label(Emotion) for train data(csv)\n",
    "3. tweets_DM: data contain score, date hashtag, tweetid and text data from twitter(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_identification = pd.read_csv(\"data_identification.csv\")\n",
    "emotion = pd.read_csv(\"emotion.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id identification\n",
      "0        0x28cc61           test\n",
      "1        0x29e452          train\n",
      "2        0x2b3819          train\n",
      "3        0x2db41f           test\n",
      "4        0x2a2acc          train\n",
      "...           ...            ...\n",
      "1867530  0x227e25          train\n",
      "1867531  0x293813          train\n",
      "1867532  0x1e1a7e          train\n",
      "1867533  0x2156a5          train\n",
      "1867534  0x2bb9d2          train\n",
      "\n",
      "[1867535 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_identification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         tweet_id       emotion\n",
      "0        0x3140b1       sadness\n",
      "1        0x368b73       disgust\n",
      "2        0x296183  anticipation\n",
      "3        0x2bd6e1           joy\n",
      "4        0x2ee1dd  anticipation\n",
      "...           ...           ...\n",
      "1455558  0x38dba0           joy\n",
      "1455559  0x300ea2           joy\n",
      "1455560  0x360b99          fear\n",
      "1455561  0x22eecf           joy\n",
      "1455562  0x2fb282  anticipation\n",
      "\n",
      "[1455563 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(emotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for json data we need to normalize (split it into seperate columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tweets_DM = pd.read_json (r'tweets_DM.json',lines=True,orient='columns')\n",
    "#normalize json data\n",
    "data = tweets_DM['_source'].tolist()\n",
    "df = pd.DataFrame.from_dict(pd.json_normalize(data), orient='columns')\n",
    "tweets_DM= pd.concat([tweets_DM, df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         _score          _index  \\\n",
      "0           391  hashtag_tweets   \n",
      "1           433  hashtag_tweets   \n",
      "2           232  hashtag_tweets   \n",
      "3           376  hashtag_tweets   \n",
      "4           989  hashtag_tweets   \n",
      "...         ...             ...   \n",
      "1867530     827  hashtag_tweets   \n",
      "1867531     368  hashtag_tweets   \n",
      "1867532     498  hashtag_tweets   \n",
      "1867533     840  hashtag_tweets   \n",
      "1867534     360  hashtag_tweets   \n",
      "\n",
      "                                                                                                                                                                                                                  _source  \\\n",
      "0                                                              {'tweet': {'hashtags': ['Snapchat'], 'tweet_id': '0x376b20', 'text': 'People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that's <LH>'}}   \n",
      "1          {'tweet': {'hashtags': ['freepress', 'TrumpLegacy', 'CNN'], 'tweet_id': '0x2d5350', 'text': '@brianklaas As we see, Trump is dangerous to #freepress around the world. What a <LH> <LH> #TrumpLegacy.  #CNN'}}   \n",
      "2        {'tweet': {'hashtags': ['bibleverse'], 'tweet_id': '0x28b412', 'text': 'Confident of your obedience, I write to you, knowing that you will do even more than I ask. (Philemon 1:21) 3/4 #bibleverse <LH> <LH>'}}   \n",
      "3                                                                                                                      {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5b0', 'text': 'Now ISSA is stalking Tasha 😂😂😂 <LH>'}}   \n",
      "4                  {'tweet': {'hashtags': [], 'tweet_id': '0x2de201', 'text': '\"Trust is not the same as faith. A friend is someone you trust. Putting faith in anyone is a mistake.\" ~ Christopher Hitchens <LH> <LH>'}}   \n",
      "...                                                                                                                                                                                                                   ...   \n",
      "1867530        {'tweet': {'hashtags': ['mixedfeeling', 'butimTHATperson'], 'tweet_id': '0x316b80', 'text': 'When you buy the last 2 tickets remaining for a show and sell it out.. #mixedfeeling <LH> #butimTHATperson'}}   \n",
      "1867531                                                                                            {'tweet': {'hashtags': [], 'tweet_id': '0x29d0cb', 'text': 'I swear all this hard work gone pay off one day😈💰💸 <LH>'}}   \n",
      "1867532                                                               {'tweet': {'hashtags': [], 'tweet_id': '0x2a6a4f', 'text': '@Parcel2Go no card left when I wasn't in so I have no idea how to get my parcel <LH>'}}   \n",
      "1867533                             {'tweet': {'hashtags': [], 'tweet_id': '0x24faed', 'text': 'Ah, corporate life, where you can date <LH> using just the relative anachronism of the last job title that updated it.'}}   \n",
      "1867534                                                                                                {'tweet': {'hashtags': ['Sundayvibes'], 'tweet_id': '0x34be8c', 'text': 'Blessed to be living #Sundayvibes <LH>'}}   \n",
      "\n",
      "                  _crawldate   _type                   tweet.hashtags  \\\n",
      "0        2015-05-23 11:42:47  tweets                       [Snapchat]   \n",
      "1        2016-01-28 04:52:09  tweets    [freepress, TrumpLegacy, CNN]   \n",
      "2        2017-12-25 04:39:20  tweets                     [bibleverse]   \n",
      "3        2016-01-24 23:53:05  tweets                               []   \n",
      "4        2016-01-08 17:18:59  tweets                               []   \n",
      "...                      ...     ...                              ...   \n",
      "1867530  2015-05-12 12:51:52  tweets  [mixedfeeling, butimTHATperson]   \n",
      "1867531  2017-10-02 17:54:04  tweets                               []   \n",
      "1867532  2016-10-10 11:04:32  tweets                               []   \n",
      "1867533  2016-09-02 14:25:06  tweets                               []   \n",
      "1867534  2016-11-16 01:40:07  tweets                    [Sundayvibes]   \n",
      "\n",
      "        tweet.tweet_id  \\\n",
      "0             0x376b20   \n",
      "1             0x2d5350   \n",
      "2             0x28b412   \n",
      "3             0x1cd5b0   \n",
      "4             0x2de201   \n",
      "...                ...   \n",
      "1867530       0x316b80   \n",
      "1867531       0x29d0cb   \n",
      "1867532       0x2a6a4f   \n",
      "1867533       0x24faed   \n",
      "1867534       0x34be8c   \n",
      "\n",
      "                                                                                                                                      tweet.text  \n",
      "0                                                              People who post \"add me on #Snapchat\" must be dehydrated. Cuz man.... that's <LH>  \n",
      "1                                 @brianklaas As we see, Trump is dangerous to #freepress around the world. What a <LH> <LH> #TrumpLegacy.  #CNN  \n",
      "2          Confident of your obedience, I write to you, knowing that you will do even more than I ask. (Philemon 1:21) 3/4 #bibleverse <LH> <LH>  \n",
      "3                                                                                                            Now ISSA is stalking Tasha 😂😂😂 <LH>  \n",
      "4        \"Trust is not the same as faith. A friend is someone you trust. Putting faith in anyone is a mistake.\" ~ Christopher Hitchens <LH> <LH>  \n",
      "...                                                                                                                                          ...  \n",
      "1867530                               When you buy the last 2 tickets remaining for a show and sell it out.. #mixedfeeling <LH> #butimTHATperson  \n",
      "1867531                                                                                  I swear all this hard work gone pay off one day😈💰💸 <LH>  \n",
      "1867532                                                     @Parcel2Go no card left when I wasn't in so I have no idea how to get my parcel <LH>  \n",
      "1867533                   Ah, corporate life, where you can date <LH> using just the relative anachronism of the last job title that updated it.  \n",
      "1867534                                                                                                   Blessed to be living #Sundayvibes <LH>  \n",
      "\n",
      "[1867535 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(tweets_DM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress size for saving memory space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        dtype_name = df[col].dtype.name\n",
    "        if dtype_name == 'object':\n",
    "            pass\n",
    "        elif dtype_name == 'bool':\n",
    "            df[col] = df[col].astype('int8')\n",
    "        elif dtype_name.startswith('int') or (df[col].round() == df[col]).all():\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('{:.1f}% Compressed'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.4% Compressed\n",
      "0.0% Compressed\n",
      "0.0% Compressed\n"
     ]
    }
   ],
   "source": [
    "tweets_DM = downcast(tweets_DM)\n",
    "data_identification = downcast(data_identification)\n",
    "emotion = downcast(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867530</th>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867531</th>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867532</th>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1867535 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id identification\n",
       "0        0x28cc61           test\n",
       "1        0x29e452          train\n",
       "2        0x2b3819          train\n",
       "3        0x2db41f           test\n",
       "4        0x2a2acc          train\n",
       "...           ...            ...\n",
       "1867530  0x227e25          train\n",
       "1867531  0x293813          train\n",
       "1867532  0x1e1a7e          train\n",
       "1867533  0x2156a5          train\n",
       "1867534  0x2bb9d2          train\n",
       "\n",
       "[1867535 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column\n",
    "tweets_DM= tweets_DM.rename(columns={'tweet.tweet_id': 'id', 'tweet.text': 'text','tweet.hashtags':'hashtags'})\n",
    "#Drop unused columns\n",
    "tweets_DM=tweets_DM.drop(['_source','_index','_type','_score', '_crawldate','hashtags'], axis=1)\n",
    "data_identification= data_identification.rename(columns={'tweet_id': 'id'})\n",
    "emotion= emotion.rename(columns={'tweet_id': 'id'})\n",
    "#put it in new dataframe\n",
    "Text = tweets_DM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing train and test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 4)\n",
      "(411972, 3)\n"
     ]
    }
   ],
   "source": [
    "traindata=data_identification[data_identification['identification']=='train']\n",
    "traindata = pd.merge(traindata, Text,  how='left', left_on=['id'], right_on = ['id'])\n",
    "traindata = pd.merge(traindata, emotion,  how='left', left_on=['id'], right_on = ['id'])\n",
    "Testdata=data_identification[data_identification['identification']=='test']\n",
    "Testdata = pd.merge(Testdata, Text,  how='left', left_on=['id'], right_on = ['id'])\n",
    "\n",
    "print(traindata.shape)\n",
    "print(Testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1455563, 4)\n",
      "(411972, 3)\n"
     ]
    }
   ],
   "source": [
    "# shuffle dataset\n",
    "traindata = traindata.sample(frac=1)\n",
    "Testdata = Testdata.sample(frac=1)\n",
    "print(traindata.shape)\n",
    "print(Testdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save data in Pickle format to continue without doing the previous step againn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.to_pickle(\"traindata.pkl\")\n",
    "Testdata.to_pickle(\"Testdata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resume from saved file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "traindata = pd.read_pickle(\"traindata.pkl\")\n",
    "Testdata = pd.read_pickle(\"Testdata.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is missing values in both train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                      identification  \\\n",
       "0  The amoung of missing records is:   The amoung of missing records is:    \n",
       "1                                   0                                   0   \n",
       "\n",
       "                                 text                             emotion  \n",
       "0  The amoung of missing records is:   The amoung of missing records is:   \n",
       "1                                   0                                   0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import helpers.data_mining_helpers as dmh\n",
    "traindata.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "      <td>The amoung of missing records is:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id                      identification  \\\n",
       "0  The amoung of missing records is:   The amoung of missing records is:    \n",
       "1                                   0                                   0   \n",
       "\n",
       "                                 text  \n",
       "0  The amoung of missing records is:   \n",
       "1                                   0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testdata.isnull().apply(lambda x: dmh.check_missing_values(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Cleaning text from tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see text lenght and words count for each tweet before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['textlen'] = [len(t) for t in traindata['text']]\n",
    "Testdata['textlen'] = [len(t) for t in Testdata['text']]\n",
    "traindata['word_count'] = traindata['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "Testdata['word_count'] = Testdata['text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>textlen</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355821</th>\n",
       "      <td>0x2e3ded</td>\n",
       "      <td>train</td>\n",
       "      <td>Traveling 2 hours to lose the way we did.... disappointed to say the least. @tombradshaw92 scored  then got taken off?? #confused.com</td>\n",
       "      <td>sadness</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89944</th>\n",
       "      <td>0x333a3d</td>\n",
       "      <td>train</td>\n",
       "      <td>Melatonin you are my friend and the only reason I sleep 😴 #insomnia #bipolar #rapidcycling &lt;LH&gt; #mentalhealth 😢</td>\n",
       "      <td>anger</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062540</th>\n",
       "      <td>0x27444d</td>\n",
       "      <td>train</td>\n",
       "      <td>@vivchook Well, get me one too! 😂 Rudeness seems so much easier for some on social media. &lt;LH&gt;</td>\n",
       "      <td>sadness</td>\n",
       "      <td>94</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190885</th>\n",
       "      <td>0x2b0378</td>\n",
       "      <td>train</td>\n",
       "      <td>@ellisonprinting Thanks Jane ♥ husband in hospital.  Busy up &amp; down. Show @deanclough  staying #optimistic.  Hope all good with you.</td>\n",
       "      <td>joy</td>\n",
       "      <td>132</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649939</th>\n",
       "      <td>0x22c7fc</td>\n",
       "      <td>train</td>\n",
       "      <td>Hey @realDonaldTrump when ur done marginalizing the &lt;LH&gt; #MilitaryBan &amp; #Pardoning #JoeArpaio #Texas might need you #HurricaneHarvey &lt;LH&gt;</td>\n",
       "      <td>disgust</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642098</th>\n",
       "      <td>0x2ef5e1</td>\n",
       "      <td>train</td>\n",
       "      <td>Phil Parker drops two deep. Huh? Blittz! It was 3rd and 12 @IowaFootball. &lt;LH&gt;</td>\n",
       "      <td>sadness</td>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437639</th>\n",
       "      <td>0x36951c</td>\n",
       "      <td>train</td>\n",
       "      <td>\"If you God is dead\" ~ #BrookBenton 😍 #MyTop10At10 😊 WOW!!!!! My childhood days were filled with joyous moments. Thank you @tboseZA &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>136</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108067</th>\n",
       "      <td>0x38222c</td>\n",
       "      <td>train</td>\n",
       "      <td>Oh my god got one almighty headache this morning &lt;LH&gt; #headache</td>\n",
       "      <td>disgust</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333393</th>\n",
       "      <td>0x249c4e</td>\n",
       "      <td>train</td>\n",
       "      <td>God has never failed me!!! &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029055</th>\n",
       "      <td>0x25f723</td>\n",
       "      <td>train</td>\n",
       "      <td>The upcoming four day weekend has me feeling &lt;LH&gt; and #humble</td>\n",
       "      <td>joy</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id identification  \\\n",
       "355821   0x2e3ded          train   \n",
       "89944    0x333a3d          train   \n",
       "1062540  0x27444d          train   \n",
       "1190885  0x2b0378          train   \n",
       "649939   0x22c7fc          train   \n",
       "...           ...            ...   \n",
       "642098   0x2ef5e1          train   \n",
       "1437639  0x36951c          train   \n",
       "108067   0x38222c          train   \n",
       "333393   0x249c4e          train   \n",
       "1029055  0x25f723          train   \n",
       "\n",
       "                                                                                                                                              text  \\\n",
       "355821       Traveling 2 hours to lose the way we did.... disappointed to say the least. @tombradshaw92 scored  then got taken off?? #confused.com   \n",
       "89944                              Melatonin you are my friend and the only reason I sleep 😴 #insomnia #bipolar #rapidcycling <LH> #mentalhealth 😢   \n",
       "1062540                                             @vivchook Well, get me one too! 😂 Rudeness seems so much easier for some on social media. <LH>   \n",
       "1190885       @ellisonprinting Thanks Jane ♥ husband in hospital.  Busy up & down. Show @deanclough  staying #optimistic.  Hope all good with you.   \n",
       "649939   Hey @realDonaldTrump when ur done marginalizing the <LH> #MilitaryBan & #Pardoning #JoeArpaio #Texas might need you #HurricaneHarvey <LH>   \n",
       "...                                                                                                                                            ...   \n",
       "642098                                                              Phil Parker drops two deep. Huh? Blittz! It was 3rd and 12 @IowaFootball. <LH>   \n",
       "1437639   \"If you God is dead\" ~ #BrookBenton 😍 #MyTop10At10 😊 WOW!!!!! My childhood days were filled with joyous moments. Thank you @tboseZA <LH>   \n",
       "108067                                                                             Oh my god got one almighty headache this morning <LH> #headache   \n",
       "333393                                                                                                             God has never failed me!!! <LH>   \n",
       "1029055                                                                              The upcoming four day weekend has me feeling <LH> and #humble   \n",
       "\n",
       "         emotion  textlen  word_count  \n",
       "355821   sadness      133          22  \n",
       "89944      anger      111          18  \n",
       "1062540  sadness       94          18  \n",
       "1190885      joy      132          23  \n",
       "649939   disgust      137          18  \n",
       "...          ...      ...         ...  \n",
       "642098   sadness       78          14  \n",
       "1437639      joy      136          23  \n",
       "108067   disgust       63          11  \n",
       "333393       joy       31           6  \n",
       "1029055      joy       61          11  \n",
       "\n",
       "[1455563 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Function to clean text from tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because texts from tweets is too messy so i'll do some cleaning before doing EDA\n",
    "These are the following steps that I do for text cleaning:\n",
    "\n",
    "0. Replace emoticon with text\n",
    "1. Replace emoji with text\n",
    "2. Remove whitespace/url/mentions/hashtags/HTML/multiple letter/special letter\n",
    "3. Lowercase\n",
    "4. Replace contractions/slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For emoticon, contraction, and slang  I found a list of both things from github and I will use them to replace the original text(credit: https://github.com/Deffro/text-preprocessing-techniques/blob/master/techniques.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined emoticon\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defined contractions/slangs\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"u\":\"you\",\n",
    "        \"r\":\"are\",\n",
    "        \"sux\":\"sucks\",\n",
    "        \"2day\":\"today\",\n",
    "        \"2nite\":\"tonight\",\n",
    "        \"4u\":\"for you\",\n",
    "        \"4ward\":\"forward\",\n",
    "        \"a3\":\"anyplace, anywhere, anytime\",\n",
    "        \"a/n\":\"author note\",\n",
    "        \"a/w\":\"anyway\",\n",
    "        \"a/s/l\":\"age, sex, location\",\n",
    "        \"adn\":\"any day now\",\n",
    "        \"afaic\":\"as far as i'm concerned\",\n",
    "        \"afaik\":\"as far as I know\",\n",
    "        \"afk\":\"away from keyboard\",\n",
    "        \"aggro\":\"aggresive\",\n",
    "        \"aight\":\"alright\",\n",
    "        \"airhead\":\"stupid\",\n",
    "        \"aka\":\"as known as\",\n",
    "        \"alol\":\"actually laughing out loud\",\n",
    "        \"amigo\":\"friend\",\n",
    "        \"amz\":\"amazing\",\n",
    "        \"app\":\"application\",\n",
    "        \"armpit\":\"undesirable\",\n",
    "        \"asap\":\"as soon as possible\",\n",
    "        \"atm\":\"at the moment\",\n",
    "        \"atw\":\"all the way\",\n",
    "        \"b/c\":\"because\",\n",
    "        \"b-day\":\"birthday\",\n",
    "        \"b4\":\"before\",\n",
    "        \"b4n\":\"bye for now\",\n",
    "        \"bae\":\"before anyone else\",\n",
    "        \"bak\":\"back at the keyboard\",\n",
    "        \"bbl\":\"bee back later\",\n",
    "        \"bday\":\"birthday\",\n",
    "        \"becuz\":\"because\",\n",
    "        \"bent\":\"angry\",\n",
    "        \"bestie\":\"best friend\",\n",
    "        \"besty\":\"best friend\",\n",
    "        \"bf\":\"boyfriend\",\n",
    "        \"bff\":\"best friends forever\",\n",
    "        \"bffe\":\"best friends forever\",\n",
    "        \"bfn\":\"bye for now\",\n",
    "        \"bg\":\"big grin\",\n",
    "        \"bmfe\":\"best mates forever\",\n",
    "        \"bmfl\":\"best mates life\",\n",
    "        \"bozo\":\"idiot\",\n",
    "        \"brah\":\"friend\",\n",
    "        \"bravo\":\"well done\",\n",
    "        \"brb\":\"be right back\",\n",
    "        \"bro\":\"brother\",\n",
    "        \"bta\":\"but then again\",\n",
    "        \"btdt\":\"been there, done that\",\n",
    "        \"btr\":\"better\",\n",
    "        \"btw\":\"by the way\",\n",
    "        \"buddy\":\"friend\",\n",
    "        \"c'mon\":\"came on\",\n",
    "        \"cid crying in disgrace\":\"\",\n",
    "        \"congrats congratulations\":\"\",\n",
    "        \"copacetic excellent\":\"\",\n",
    "        \"coz beacause\":\"\",\n",
    "        \"cu\":\"see you\",\n",
    "        \"cuddy\":\"friends\",\n",
    "        \"cul\":\"see you later\",\n",
    "        \"cul8r\":\"see you later \",\n",
    "        \"cutie\":\"cute\",\n",
    "        \"cuz\":\"because\",\n",
    "        \"cya\":\"bye\",\n",
    "        \"cyo\":\"see you online \",\n",
    "        \"dbau\":\"doing business as usual\",\n",
    "        \"deets\":\"details\",\n",
    "        \"dmn\":\"damn\",\n",
    "        \"dobe\":\"idiot\",\n",
    "        \"dope\":\"stupid\",\n",
    "        \"dork\":\"strange\",\n",
    "        \"dunno\":\"don't know\",\n",
    "        \"dwi\":\"deal with it\",\n",
    "        \"dyd\":\"don't you dare\",\n",
    "        \"ermahgerd\":\"oh my gosh\",\n",
    "        \"eu\":\"europe\",\n",
    "        \"ez\":\"easy\",\n",
    "        \"f9\":\"fine\",\n",
    "        \"fav\":\"favorite\",\n",
    "        \"far-out\":\"great\",\n",
    "        \"fb\":\"facebook\",\n",
    "        \"flick\":\"movie\",\n",
    "        \"fml\":\"fuck my life\",\n",
    "        \"foxy\":\"sexy\",\n",
    "        \"friggin\":\"freaking\",\n",
    "        \"fttn\":\"for the time being\",\n",
    "        \"ftw\":\"for the win\",\n",
    "        \"fud\":\"fear, uncertainty, and doubt\",\n",
    "        \"fwiw\":\"for what it's worth \",\n",
    "        \"fyi\":\"for your information\",\n",
    "        \"g\":\"grin \",\n",
    "        \"g2g\":\"got to go \",\n",
    "        \"ga\":\"go ahead \",\n",
    "        \"gal\":\"get a life\",\n",
    "        \"getcha\":\"understand \",\n",
    "        \"gf\":\"girlfriend \",\n",
    "        \"gfn\":\"gone for now\",\n",
    "        \"gg\":\"good game\",\n",
    "        \"gj\":\"good job\",\n",
    "        \"gky\":\"go kill yourself\",\n",
    "        \"gl\":\"good luck\",\n",
    "        \"glhf\":\"good luck have fun\",\n",
    "        \"gmab\":\"give me a break\",\n",
    "        \"gmbo\":\"giggling my butt off \",\n",
    "        \"gmta\":\"great minds think alike \",\n",
    "        \"goof\":\"idiot\",\n",
    "        \"goofy\":\"idiot\",\n",
    "        \"gr8\":\"great\",\n",
    "        \"gtg\":\"got to go\",\n",
    "        \"gud\":\"good\",\n",
    "        \"h8\":\"hate\",\n",
    "        \"hagn\":\"have a good night \",\n",
    "        \"hdop\":\"help delete online predators \",\n",
    "        \"hf\":\"have fun\",\n",
    "        \"hml\":\"hate my life\",\n",
    "        \"hoas\":\"hold on a second\",\n",
    "        \"hhis\":\"hanging head in shame \",\n",
    "        \"hmu\":\"hit me up\",\n",
    "        \"hru\":\"how are you\",\n",
    "        \"twt\":\"hope this helps\",\n",
    "        \"hw\":\"homework\",\n",
    "        \"im\":\"i am\",\n",
    "        \"i'ma\":\"i am going to\",\n",
    "        \"iac\":\"in any case \",\n",
    "        \"ic\":\"I see \",\n",
    "        \"icymi\":\"in case you missed it\",\n",
    "        \"idk\":\"I don't know\",\n",
    "        \"iggy\":\"ignore \",\n",
    "        \"iht\":\"i hate this\",\n",
    "        \"ikr\":\"i know, right?\",\n",
    "        \"ilt\":\"i like that\",\n",
    "        \"ily\":\"i love you\",\n",
    "        \"ima\":\"i am going to \",\n",
    "        \"imao\":\"in my arrogant opinion\",\n",
    "        \"imnsho\":\"in my not so humble opinion \",\n",
    "        \"imo\":\"in my opinion \",\n",
    "        \"imy\":\"i miss you\",\n",
    "        \"iou\":\"i owe you\",\n",
    "        \"iow\":\"in other words \",\n",
    "        \"ipn\":\"I’m posting naked \",\n",
    "        \"irl\":\"in real life \",\n",
    "        \"j/k\":\"just kidding\",\n",
    "        \"jdi\":\"just do it\",\n",
    "        \"jk\":\"just kidding\",\n",
    "        \"jkn\":\"joking\",\n",
    "        \"jyeah\":\"yeah\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"l8\":\"late\",\n",
    "        \"l8r\":\"later\",\n",
    "        \"lbh\":\"let's be honest\",\n",
    "        \"ld\":\"later, dude\",\n",
    "        \"ldi\":\"let's do it  \",\n",
    "        \"ldr\":\"long distance relationship \",\n",
    "        \"lees\":\"beautiful \",\n",
    "        \"lfm\":\"looking for more\",\n",
    "        \"lil\":\"little\",\n",
    "        \"llta\":\"lots and lots of thunderous applause \",\n",
    "        \"lmao\":\"laugh my ass off\",\n",
    "        \"lmirl\":\"let's meet in real life \",\n",
    "        \"lmk\":\"let me know\",\n",
    "        \"lol\":\"laugh out loud\",\n",
    "        \"lolz\":\"laugh out loud\",\n",
    "        \"lotta\":\"lot of\",\n",
    "        \"lsr\":\"loser\",\n",
    "        \"ltr\":\"longterm relationship\",\n",
    "        \"lua\":\"love you always \",\n",
    "        \"lub\":\"love\",\n",
    "        \"lubb\":\"love\",\n",
    "        \"lulab\":\"love you like a brother \",\n",
    "        \"lulas\":\"love you like a sister \",\n",
    "        \"lul\":\"laugh\",\n",
    "        \"luls\":\"laugh\",\n",
    "        \"lulz\":\"laugh\",\n",
    "        \"lumu\":\"love you miss you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"lux\":\"luxury\",\n",
    "        \"lwm\":\"laugh with me\",\n",
    "        \"lwp\":\"laugh with passion\",\n",
    "        \"lvl\":\"level\",\n",
    "        \"m/f\":\"male or female\",\n",
    "        \"m2\":\"me too\",\n",
    "        \"m8\":\"mate\",\n",
    "        \"me2\":\"me too\",\n",
    "        \"milf\":\"mother I would like to fuck\",\n",
    "        \"mma\":\"meet me at\",\n",
    "        \"mmb\":\"message me back\",\n",
    "        \"mvp\":\"most valueable player\",\n",
    "        \"msg\":\"message\",\n",
    "        \"mtf\":\"more to follow\",\n",
    "        \"myob\":\"mind your own business\",\n",
    "        \"nah\":\"no\",\n",
    "        \"nc\":\"no comment\",\n",
    "        \"nk\":\"not kidding\",\n",
    "        \"ngl\":\"not gonna lie\",\n",
    "        \"nlt\":\"no later than\",\n",
    "        \"nm\":\"not much\",\n",
    "        \"no1\":\"no one\",\n",
    "        \"np\":\"no problem\",\n",
    "        \"nsfw\":\"not safe for work\",\n",
    "        \"nuh\":\"no\",\n",
    "        \"nvm\":\"nevermind\",\n",
    "        \"obo\":\"or best offer\",\n",
    "        \"oic\":\"oh, i see\",\n",
    "        \"oll\":\"online love \",\n",
    "        \"omg\":\"oh my god\",\n",
    "        \"omw\":\"on my way\",\n",
    "        \"osm\":\"awesome\",\n",
    "        \"otoh\":\"on the other hand \",\n",
    "        \"perv\":\"pervert\",\n",
    "        \"pervy\":\"pervert\",\n",
    "        \"phat\":\"pretty hot and tempting\",\n",
    "        \"pir\":\"parent in room\",\n",
    "        \"pls\":\"please\",\n",
    "        \"plz\":\"please\",\n",
    "        \"ppl\":\"people\",\n",
    "        \"pro\":\"professional\",\n",
    "        \"pwnd\":\"owned\",\n",
    "        \"qq\":\"crying\",\n",
    "        \"r\":\"are\",\n",
    "        \"rly\":\"really\",\n",
    "        \"rofl\":\"roll on the floor laughing\",\n",
    "        \"rolf\":\"roll on the floor laughing\",\n",
    "        \"rpg\":\"role playing games\",\n",
    "        \"ru\":\"are you\",\n",
    "        \"s2u\":\"shame to you\",\n",
    "        \"scrub\":\"loser\",\n",
    "        \"sec\":\"second\",\n",
    "        \"shid\":\"slaps head in disgust\",\n",
    "        \"shoulda\":\"should have\",\n",
    "        \"sff\":\"so funny\",\n",
    "        \"smexy\":\"smart and sexy\",\n",
    "        \"smh\":\"shaking my head\",\n",
    "        \"somy\":\"sick of me yet\",\n",
    "        \"sot\":\"short of time \",\n",
    "        \"sry\":\"sorry\",\n",
    "        \"str8\":\"straight\",\n",
    "        \"sux\":\"sucks\",\n",
    "        \"swag\":\"style\",\n",
    "        \"taze\":\"irritate\",\n",
    "        \"tba\":\"to be announced\",\n",
    "        \"tbfu\":\"too bad for you\",\n",
    "        \"tbc\":\"to be continued\",\n",
    "        \"tbd\":\"to be determined\",\n",
    "        \"tbr\":\"to be rude\",\n",
    "        \"tc\":\"take care\",\n",
    "        \"thx\":\"thanks\",\n",
    "        \"thanx\":\"thanks\",\n",
    "        \"thx\":\"thanks\",\n",
    "        \"tfw\":\"that feeling  when\",\n",
    "        \"til\":\"today i learned\",\n",
    "        \"ttyl\":\"talk to you later \",\n",
    "        \"ty\":\"thank you\",\n",
    "        \"tyvm\":\"thank you very much\",\n",
    "        \"u\":\"you\",\n",
    "        \"uber\":\"the best\",\n",
    "        \"ugh\":\"disgusted\",\n",
    "        \"ur\":\"you are\",\n",
    "        \"uw\":\"you are welcome \",\n",
    "        \"vs\":\"versus\",\n",
    "        \"w2f\":\"way too funny\",\n",
    "        \"w8\":\"wait\",\n",
    "        \"wak\":\"weird\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wb\":\"welcome back\",\n",
    "        \"whiz\":\"talented\",\n",
    "        \"whoa\":\"surprise\",\n",
    "        \"whoah\":\"surprise\",\n",
    "        \"wfm\":\"works for me \",\n",
    "        \"wibni\":\"wouldn't it be nice if \",\n",
    "        \"wmd\":\"weapon of mass destruction\",\n",
    "        \"wot\":\"what\",\n",
    "        \"wtf\":\"what the fuck\",\n",
    "        \"wtg\":\"way to go\",\n",
    "        \"wtgp\":\"want to go private\",\n",
    "        \"wu\":\"what's up\",\n",
    "        \"wuh\":\"what?\",\n",
    "        \"wuv\":\"love\",\n",
    "        \"ym\":\"young man\",\n",
    "        \"yawn\":\"boring\",\n",
    "        \"yum\":\"good\",\n",
    "        \"x\":\"kiss\",\n",
    "        \"xxx\":\"kiss\",\n",
    "        \"xdd\":\"laughing\",\n",
    "        \"y\":\"why\",\n",
    "        \"yolo\":\"you only live once\",\n",
    "        \"yuge\":\"huge\",\n",
    "        \"yw\":\"you are welcome\",\n",
    "        \"ywa\":\"you are welcome anyway\",\n",
    "        \"zomg\":\"oh my god!\",\n",
    "        \"zzz\":\"sleeping\"    \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace emoticon with words in the dictionary\n",
    "emoticon=load_dict_smileys()\n",
    "def demoticon(text):\n",
    "    text = text.replace(\"’\",\"'\")\n",
    "    words = text.split()\n",
    "    text = [emoticon[text] if text in emoticon else text for text in words]\n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pypi.org/project/emoji/\n",
    "#if ImportError: cannot import name 'UNICODE_EMOJI' from 'emoji', reinstall below.\n",
    "#pip uninstall emoji\n",
    "#https://anaconda.org/conda-forge/emoji\n",
    "#conda install -c conda-forge emoji==1.6.3\n",
    "\n",
    "import emoji\n",
    "from emoji import emojize\n",
    "# add space before and after the emoji\n",
    "from emoji import UNICODE_EMOJI\n",
    "def is_emoji(s, language=\"en\"):\n",
    "    return s in UNICODE_EMOJI[language]\n",
    "    \n",
    "def add_space(text):\n",
    "    return ''.join(' ' + char + ' 'if is_emoji(char) else char for char in text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace emoji using emoji library\n",
    "def demoji(text):\n",
    "    text = emoji.demojize(text, delimiters=(\"\", \" \"))  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean noisy data\n",
    "import re\n",
    "import itertools\n",
    "def cleaning_text(text):    \n",
    "    text = text.strip() # remove white space\n",
    "    text = re.sub(r'http\\S+', \"\", text)    # remove urls\n",
    "    text = re.sub(r'@\\w+','',text)         # remove mentions\n",
    "    text = re.sub(r'#', '', text)       # remove hastags \\w+\n",
    "    text = re.sub(r'<\\w+','', text)       # remove html tags\n",
    "    text = re.sub(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', '', text) \n",
    "    text = re.sub(r'>', '', text)\n",
    "    text = ''.join(''.join(s)[:2] for _, s in itertools.groupby(text)) # remove multiple letters\n",
    "    text = text.lower()  #convert to lower case\n",
    "    text = re.sub('[!,*)@#%(&$_?.^+->:;/\\\"]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the contractions\n",
    "CONTRACTIONS=load_dict_contractions()\n",
    "def replaceconslang(text):\n",
    "    text = text.replace(\"’\",\"'\")\n",
    "    words = text.split()\n",
    "    text = [CONTRACTIONS[text] if text in CONTRACTIONS else text for text in words]\n",
    "    text = \" \".join(text)\n",
    "    text =re.sub('[^a-zA-Z0-9 \\n\\.]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply function to dataframe & insert into new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 2s\n",
      "Wall time: 3min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "traindata['cleantext']=traindata['text'].apply(lambda x: demoticon(x))\n",
    "Testdata['cleantext']=Testdata['text'].apply(lambda x: demoticon(x))\n",
    "traindata['cleantext']=traindata['cleantext'].apply(lambda x: add_space(x))\n",
    "Testdata['cleantext']=Testdata['cleantext'].apply(lambda x: add_space(x))\n",
    "traindata['cleantext']=traindata['cleantext'].apply(lambda x: demoji(x))\n",
    "Testdata['cleantext']=Testdata['cleantext'].apply(lambda x: demoji(x))\n",
    "traindata['cleantext']=traindata['cleantext'].apply(lambda x: cleaning_text(x))\n",
    "Testdata['cleantext']=Testdata['cleantext'].apply(lambda x: cleaning_text(x))\n",
    "traindata['cleantext']=traindata['cleantext'].apply(lambda x: replaceconslang(x))\n",
    "Testdata['cleantext']=Testdata['cleantext'].apply(lambda x: replaceconslang(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['textlenafter']=[len(t) for t in traindata['cleantext']]\n",
    "traindata['word_countafter'] = traindata['cleantext'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>textlen</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>textlenafter</th>\n",
       "      <th>word_countafter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355821</th>\n",
       "      <td>0x2e3ded</td>\n",
       "      <td>train</td>\n",
       "      <td>Traveling 2 hours to lose the way we did.... disappointed to say the least. @tombradshaw92 scored  then got taken off?? #confused.com</td>\n",
       "      <td>sadness</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>traveling hours to lose the way we did disappointed to say the least scored then got taken off confusedcom</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89944</th>\n",
       "      <td>0x333a3d</td>\n",
       "      <td>train</td>\n",
       "      <td>Melatonin you are my friend and the only reason I sleep 😴 #insomnia #bipolar #rapidcycling &lt;LH&gt; #mentalhealth 😢</td>\n",
       "      <td>anger</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "      <td>melatonin you are my friend and the only reason i sleep sleepingface insomnia bipolar rapidcycling mentalhealth cryingface</td>\n",
       "      <td>122</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062540</th>\n",
       "      <td>0x27444d</td>\n",
       "      <td>train</td>\n",
       "      <td>@vivchook Well, get me one too! 😂 Rudeness seems so much easier for some on social media. &lt;LH&gt;</td>\n",
       "      <td>sadness</td>\n",
       "      <td>94</td>\n",
       "      <td>18</td>\n",
       "      <td>well get me one too facewithtearsofjoy rudeness seems so much easier for some on social media</td>\n",
       "      <td>93</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190885</th>\n",
       "      <td>0x2b0378</td>\n",
       "      <td>train</td>\n",
       "      <td>@ellisonprinting Thanks Jane ♥ husband in hospital.  Busy up &amp; down. Show @deanclough  staying #optimistic.  Hope all good with you.</td>\n",
       "      <td>joy</td>\n",
       "      <td>132</td>\n",
       "      <td>23</td>\n",
       "      <td>thanks jane heartsuit husband in hospital busy up down show staying optimistic hope all good with you</td>\n",
       "      <td>101</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649939</th>\n",
       "      <td>0x22c7fc</td>\n",
       "      <td>train</td>\n",
       "      <td>Hey @realDonaldTrump when ur done marginalizing the &lt;LH&gt; #MilitaryBan &amp; #Pardoning #JoeArpaio #Texas might need you #HurricaneHarvey &lt;LH&gt;</td>\n",
       "      <td>disgust</td>\n",
       "      <td>137</td>\n",
       "      <td>18</td>\n",
       "      <td>hey when you are done marginalizing the militaryban pardoning joearpaio texas might need you hurricaneharvey</td>\n",
       "      <td>108</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642098</th>\n",
       "      <td>0x2ef5e1</td>\n",
       "      <td>train</td>\n",
       "      <td>Phil Parker drops two deep. Huh? Blittz! It was 3rd and 12 @IowaFootball. &lt;LH&gt;</td>\n",
       "      <td>sadness</td>\n",
       "      <td>78</td>\n",
       "      <td>14</td>\n",
       "      <td>phil parker drops two deep huh blittz it was rd and</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437639</th>\n",
       "      <td>0x36951c</td>\n",
       "      <td>train</td>\n",
       "      <td>\"If you God is dead\" ~ #BrookBenton 😍 #MyTop10At10 😊 WOW!!!!! My childhood days were filled with joyous moments. Thank you @tboseZA &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>136</td>\n",
       "      <td>23</td>\n",
       "      <td>if you god is dead  brookbenton smilingfacewithhearteyes mytopat smilingfacewithsmilingeyes wow my childhood days were filled with joyous moments thank you</td>\n",
       "      <td>155</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108067</th>\n",
       "      <td>0x38222c</td>\n",
       "      <td>train</td>\n",
       "      <td>Oh my god got one almighty headache this morning &lt;LH&gt; #headache</td>\n",
       "      <td>disgust</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>oh my god got one almighty headache this morning headache</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333393</th>\n",
       "      <td>0x249c4e</td>\n",
       "      <td>train</td>\n",
       "      <td>God has never failed me!!! &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>god has never failed me</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029055</th>\n",
       "      <td>0x25f723</td>\n",
       "      <td>train</td>\n",
       "      <td>The upcoming four day weekend has me feeling &lt;LH&gt; and #humble</td>\n",
       "      <td>joy</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>the upcoming four day weekend has me feeling and humble</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id identification  \\\n",
       "355821   0x2e3ded          train   \n",
       "89944    0x333a3d          train   \n",
       "1062540  0x27444d          train   \n",
       "1190885  0x2b0378          train   \n",
       "649939   0x22c7fc          train   \n",
       "...           ...            ...   \n",
       "642098   0x2ef5e1          train   \n",
       "1437639  0x36951c          train   \n",
       "108067   0x38222c          train   \n",
       "333393   0x249c4e          train   \n",
       "1029055  0x25f723          train   \n",
       "\n",
       "                                                                                                                                              text  \\\n",
       "355821       Traveling 2 hours to lose the way we did.... disappointed to say the least. @tombradshaw92 scored  then got taken off?? #confused.com   \n",
       "89944                              Melatonin you are my friend and the only reason I sleep 😴 #insomnia #bipolar #rapidcycling <LH> #mentalhealth 😢   \n",
       "1062540                                             @vivchook Well, get me one too! 😂 Rudeness seems so much easier for some on social media. <LH>   \n",
       "1190885       @ellisonprinting Thanks Jane ♥ husband in hospital.  Busy up & down. Show @deanclough  staying #optimistic.  Hope all good with you.   \n",
       "649939   Hey @realDonaldTrump when ur done marginalizing the <LH> #MilitaryBan & #Pardoning #JoeArpaio #Texas might need you #HurricaneHarvey <LH>   \n",
       "...                                                                                                                                            ...   \n",
       "642098                                                              Phil Parker drops two deep. Huh? Blittz! It was 3rd and 12 @IowaFootball. <LH>   \n",
       "1437639   \"If you God is dead\" ~ #BrookBenton 😍 #MyTop10At10 😊 WOW!!!!! My childhood days were filled with joyous moments. Thank you @tboseZA <LH>   \n",
       "108067                                                                             Oh my god got one almighty headache this morning <LH> #headache   \n",
       "333393                                                                                                             God has never failed me!!! <LH>   \n",
       "1029055                                                                              The upcoming four day weekend has me feeling <LH> and #humble   \n",
       "\n",
       "         emotion  textlen  word_count  \\\n",
       "355821   sadness      133          22   \n",
       "89944      anger      111          18   \n",
       "1062540  sadness       94          18   \n",
       "1190885      joy      132          23   \n",
       "649939   disgust      137          18   \n",
       "...          ...      ...         ...   \n",
       "642098   sadness       78          14   \n",
       "1437639      joy      136          23   \n",
       "108067   disgust       63          11   \n",
       "333393       joy       31           6   \n",
       "1029055      joy       61          11   \n",
       "\n",
       "                                                                                                                                                           cleantext  \\\n",
       "355821                                                    traveling hours to lose the way we did disappointed to say the least scored then got taken off confusedcom   \n",
       "89944                                     melatonin you are my friend and the only reason i sleep sleepingface insomnia bipolar rapidcycling mentalhealth cryingface   \n",
       "1062540                                                                well get me one too facewithtearsofjoy rudeness seems so much easier for some on social media   \n",
       "1190885                                                        thanks jane heartsuit husband in hospital busy up down show staying optimistic hope all good with you   \n",
       "649939                                                  hey when you are done marginalizing the militaryban pardoning joearpaio texas might need you hurricaneharvey   \n",
       "...                                                                                                                                                              ...   \n",
       "642098                                                                                                           phil parker drops two deep huh blittz it was rd and   \n",
       "1437639  if you god is dead  brookbenton smilingfacewithhearteyes mytopat smilingfacewithsmilingeyes wow my childhood days were filled with joyous moments thank you   \n",
       "108067                                                                                                     oh my god got one almighty headache this morning headache   \n",
       "333393                                                                                                                                       god has never failed me   \n",
       "1029055                                                                                                      the upcoming four day weekend has me feeling and humble   \n",
       "\n",
       "         textlenafter  word_countafter  \n",
       "355821            106               19  \n",
       "89944             122               17  \n",
       "1062540            93               16  \n",
       "1190885           101               17  \n",
       "649939            108               15  \n",
       "...               ...              ...  \n",
       "642098             51               11  \n",
       "1437639           155               21  \n",
       "108067             57               10  \n",
       "333393             23                5  \n",
       "1029055            55               10  \n",
       "\n",
       "[1455563 rows x 9 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i tried English Language Detection use fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Building fastText for Python\n",
    "For now this is not part of a release, so you will need to clone the master branch.\n",
    "\n",
    "$ git clone https://github.com/facebookresearch/fastText.git\n",
    "$ cd fastText\n",
    "$ pip install .\n",
    "\n",
    "if happen below error\n",
    "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "You should follow below instruction to install\n",
    "https://www.scivision.dev/python-windows-visual-c-14-required\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import fasttext\n",
    "pretrained_model = \"lid.176.bin\"  #download from https://fasttext.cc/docs/en/language-identification.html\n",
    "model = fasttext.load_model(pretrained_model)\n",
    "langs = []\n",
    "for sent in traindata['cleantext']:\n",
    "    lang = model.predict(sent)[0]\n",
    "    langs.append(str(lang)[:])\n",
    "traindata['langs'] = langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>textlen</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>textlenafter</th>\n",
       "      <th>word_countafter</th>\n",
       "      <th>langs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154136</th>\n",
       "      <td>0x2f0169</td>\n",
       "      <td>train</td>\n",
       "      <td>@raymundop_ Wow que sexy boy .....#cool</td>\n",
       "      <td>joy</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>wow que sexy boy cool</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>('__label__ca',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741070</th>\n",
       "      <td>0x37952f</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;LH&gt; Oh lawd,Molly!!</td>\n",
       "      <td>fear</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>oh lawdmolly</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>('__label__es',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560478</th>\n",
       "      <td>0x37deae</td>\n",
       "      <td>train</td>\n",
       "      <td>I love God &lt;LH&gt;</td>\n",
       "      <td>trust</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>i love god</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>('__label__sv',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181224</th>\n",
       "      <td>0x2a8dbd</td>\n",
       "      <td>train</td>\n",
       "      <td>@LENALittleClosa &lt;LH&gt; lmfao  @HawaiianChuch_</td>\n",
       "      <td>trust</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>lmfao</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>('__label__ar',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304754</th>\n",
       "      <td>0x216e46</td>\n",
       "      <td>train</td>\n",
       "      <td>Honestly I'm just ready for &lt;LH&gt; now</td>\n",
       "      <td>fear</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>honestly im just ready for now</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>('__label__de',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1274537</th>\n",
       "      <td>0x2c9710</td>\n",
       "      <td>train</td>\n",
       "      <td>@July_Mo &lt;LH&gt; x Pennywise</td>\n",
       "      <td>joy</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>kiss pennywise</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>('__label__nl',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659144</th>\n",
       "      <td>0x29e830</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;LH&gt; heel say ‘watch @JohnTorode1 on @GoodFoodChannel’ @markgreenaway @nigelbrownchef @TheCurryGuy @chefgingernut @ChefJeremyPang</td>\n",
       "      <td>sadness</td>\n",
       "      <td>129</td>\n",
       "      <td>12</td>\n",
       "      <td>heel say watch on</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>('__label__nl',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106579</th>\n",
       "      <td>0x31ba29</td>\n",
       "      <td>train</td>\n",
       "      <td>Wurstel for lunch!Even if a want to bad to tey bratwurst... #lunch &lt;LH&gt; #meat #wurstel</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>86</td>\n",
       "      <td>15</td>\n",
       "      <td>wurstel for luncheven if a want to bad to tey bratwurst lunch meat wurstel</td>\n",
       "      <td>74</td>\n",
       "      <td>14</td>\n",
       "      <td>('__label__de',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593565</th>\n",
       "      <td>0x2ab477</td>\n",
       "      <td>train</td>\n",
       "      <td>Rain &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>rain</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>('__label__ja',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568309</th>\n",
       "      <td>0x1e60d6</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;LH&gt; Mandela Day#</td>\n",
       "      <td>joy</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>mandela day</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>('__label__fr',)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15861 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id identification  \\\n",
       "154136   0x2f0169          train   \n",
       "741070   0x37952f          train   \n",
       "560478   0x37deae          train   \n",
       "181224   0x2a8dbd          train   \n",
       "304754   0x216e46          train   \n",
       "...           ...            ...   \n",
       "1274537  0x2c9710          train   \n",
       "659144   0x29e830          train   \n",
       "106579   0x31ba29          train   \n",
       "593565   0x2ab477          train   \n",
       "568309   0x1e60d6          train   \n",
       "\n",
       "                                                                                                                                      text  \\\n",
       "154136                                                                                             @raymundop_ Wow que sexy boy .....#cool   \n",
       "741070                                                                                                                <LH> Oh lawd,Molly!!   \n",
       "560478                                                                                                                     I love God <LH>   \n",
       "181224                                                                                        @LENALittleClosa <LH> lmfao  @HawaiianChuch_   \n",
       "304754                                                                                                Honestly I'm just ready for <LH> now   \n",
       "...                                                                                                                                    ...   \n",
       "1274537                                                                                                          @July_Mo <LH> x Pennywise   \n",
       "659144   <LH> heel say ‘watch @JohnTorode1 on @GoodFoodChannel’ @markgreenaway @nigelbrownchef @TheCurryGuy @chefgingernut @ChefJeremyPang   \n",
       "106579                                              Wurstel for lunch!Even if a want to bad to tey bratwurst... #lunch <LH> #meat #wurstel   \n",
       "593565                                                                                                                           Rain <LH>   \n",
       "568309                                                                                                                   <LH> Mandela Day#   \n",
       "\n",
       "              emotion  textlen  word_count  \\\n",
       "154136            joy       39           6   \n",
       "741070           fear       20           3   \n",
       "560478          trust       15           4   \n",
       "181224          trust       44           5   \n",
       "304754           fear       36           7   \n",
       "...               ...      ...         ...   \n",
       "1274537           joy       25           4   \n",
       "659144        sadness      129          12   \n",
       "106579   anticipation       86          15   \n",
       "593565            joy        9           2   \n",
       "568309            joy       17           3   \n",
       "\n",
       "                                                                          cleantext  \\\n",
       "154136                                                        wow que sexy boy cool   \n",
       "741070                                                                 oh lawdmolly   \n",
       "560478                                                                   i love god   \n",
       "181224                                                                        lmfao   \n",
       "304754                                               honestly im just ready for now   \n",
       "...                                                                             ...   \n",
       "1274537                                                              kiss pennywise   \n",
       "659144                                                           heel say watch on    \n",
       "106579   wurstel for luncheven if a want to bad to tey bratwurst lunch meat wurstel   \n",
       "593565                                                                         rain   \n",
       "568309                                                                  mandela day   \n",
       "\n",
       "         textlenafter  word_countafter             langs  \n",
       "154136             21                5  ('__label__ca',)  \n",
       "741070             12                2  ('__label__es',)  \n",
       "560478             10                3  ('__label__sv',)  \n",
       "181224              5                1  ('__label__ar',)  \n",
       "304754             30                6  ('__label__de',)  \n",
       "...               ...              ...               ...  \n",
       "1274537            14                2  ('__label__nl',)  \n",
       "659144             18                5  ('__label__nl',)  \n",
       "106579             74               14  ('__label__de',)  \n",
       "593565              4                1  ('__label__ja',)  \n",
       "568309             11                2  ('__label__fr',)  \n",
       "\n",
       "[15861 rows x 10 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[~traindata[\"langs\"].str.contains('en')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes it detect english sentence to be in other language so I didn't remove these records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Null and remove duplicated value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>textlen</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>textlenafter</th>\n",
       "      <th>word_countafter</th>\n",
       "      <th>langs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, identification, text, emotion, textlen, word_count, cleantext, textlenafter, word_countafter, langs]\n",
       "Index: []"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[traindata.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>textlen</th>\n",
       "      <th>word_count</th>\n",
       "      <th>cleantext</th>\n",
       "      <th>textlenafter</th>\n",
       "      <th>word_countafter</th>\n",
       "      <th>langs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199832</th>\n",
       "      <td>0x2257c3</td>\n",
       "      <td>train</td>\n",
       "      <td>@V2CigsUK wow how nice &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>wow how nice</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415133</th>\n",
       "      <td>0x1e3f31</td>\n",
       "      <td>train</td>\n",
       "      <td>Closed Buy 1.4 Lots EURUSD 1.18502 for +10.0 pips, total for today +761.2 pips &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>103</td>\n",
       "      <td>19</td>\n",
       "      <td>closed buy lots eurusd for pips total for today pips</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041647</th>\n",
       "      <td>0x382e0c</td>\n",
       "      <td>train</td>\n",
       "      <td>@BradKidd33 @cajunlady64 @DeepStateAgent2 @tommyleeedwards @Deplorablelou @billoreilly &lt;LH&gt; does that even mean?</td>\n",
       "      <td>disgust</td>\n",
       "      <td>112</td>\n",
       "      <td>11</td>\n",
       "      <td>does that even mean</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307483</th>\n",
       "      <td>0x2ce068</td>\n",
       "      <td>train</td>\n",
       "      <td>Closed Buy 1.0 Lots EURUSD 1.176 for +27.7 pips, total for today +338.0 pips &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "      <td>closed buy lots eurusd for pips total for today pips</td>\n",
       "      <td>52</td>\n",
       "      <td>10</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329653</th>\n",
       "      <td>0x1d69dc</td>\n",
       "      <td>train</td>\n",
       "      <td>@NavyVets4Trump Thank you VERY much for your service and for the follow fellow Patriot, #ISALUTEYOU! &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; 🇺🇸</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>123</td>\n",
       "      <td>20</td>\n",
       "      <td>thank you very much for your service and for the follow fellow patriot isaluteyou unitedstates</td>\n",
       "      <td>94</td>\n",
       "      <td>15</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086237</th>\n",
       "      <td>0x388cd6</td>\n",
       "      <td>train</td>\n",
       "      <td>@wesway94 #faith, #hope, and &lt;LH&gt; will last forever. The greatest is #love. 1 corinthians 13:13 &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>faith hope and will last forever the greatest is love corinthians</td>\n",
       "      <td>65</td>\n",
       "      <td>11</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069156</th>\n",
       "      <td>0x382b2c</td>\n",
       "      <td>train</td>\n",
       "      <td>71 The moments in your life are only once #Life &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; &lt;LH&gt; September 06, 2017 at 08:30PM</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>112</td>\n",
       "      <td>22</td>\n",
       "      <td>the moments in your life are only once life september at pm</td>\n",
       "      <td>59</td>\n",
       "      <td>12</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362271</th>\n",
       "      <td>0x27c6da</td>\n",
       "      <td>train</td>\n",
       "      <td>#Love, like a river, will cut a new path whenever it meets an obstacle. - @RockChristopher  &lt;LH&gt; &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>101</td>\n",
       "      <td>19</td>\n",
       "      <td>love like a river will cut a new path whenever it meets an obstacle</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117577</th>\n",
       "      <td>0x2be2fa</td>\n",
       "      <td>train</td>\n",
       "      <td>Watching The Pagemaster (1994) &lt;LH&gt; #90skids #thepagemaster #bookishmovies</td>\n",
       "      <td>sadness</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>watching the pagemaster skids thepagemaster bookishmovies</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346903</th>\n",
       "      <td>0x309438</td>\n",
       "      <td>train</td>\n",
       "      <td>39 Never give up on your #dream. &lt;LH&gt; me to make your dreams come &lt;LH&gt; &lt;LH&gt; November 13, 2017 at 07:00AM</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>104</td>\n",
       "      <td>21</td>\n",
       "      <td>never give up on your dream me to make your dreams come november at am</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>('__label__en',)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>128379 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id identification  \\\n",
       "199832   0x2257c3          train   \n",
       "415133   0x1e3f31          train   \n",
       "1041647  0x382e0c          train   \n",
       "1307483  0x2ce068          train   \n",
       "1329653  0x1d69dc          train   \n",
       "...           ...            ...   \n",
       "1086237  0x388cd6          train   \n",
       "1069156  0x382b2c          train   \n",
       "362271   0x27c6da          train   \n",
       "117577   0x2be2fa          train   \n",
       "346903   0x309438          train   \n",
       "\n",
       "                                                                                                                                text  \\\n",
       "199832                                                                                                   @V2CigsUK wow how nice <LH>   \n",
       "415133                       Closed Buy 1.4 Lots EURUSD 1.18502 for +10.0 pips, total for today +761.2 pips <LH> <LH> <LH> <LH> <LH>   \n",
       "1041647             @BradKidd33 @cajunlady64 @DeepStateAgent2 @tommyleeedwards @Deplorablelou @billoreilly <LH> does that even mean?   \n",
       "1307483                        Closed Buy 1.0 Lots EURUSD 1.176 for +27.7 pips, total for today +338.0 pips <LH> <LH> <LH> <LH> <LH>   \n",
       "1329653  @NavyVets4Trump Thank you VERY much for your service and for the follow fellow Patriot, #ISALUTEYOU! <LH> <LH> <LH> <LH> 🇺🇸   \n",
       "...                                                                                                                              ...   \n",
       "1086237                         @wesway94 #faith, #hope, and <LH> will last forever. The greatest is #love. 1 corinthians 13:13 <LH>   \n",
       "1069156             71 The moments in your life are only once #Life <LH> <LH> <LH> <LH> <LH> <LH> <LH> September 06, 2017 at 08:30PM   \n",
       "362271                         #Love, like a river, will cut a new path whenever it meets an obstacle. - @RockChristopher  <LH> <LH>   \n",
       "117577                                                    Watching The Pagemaster (1994) <LH> #90skids #thepagemaster #bookishmovies   \n",
       "346903                      39 Never give up on your #dream. <LH> me to make your dreams come <LH> <LH> November 13, 2017 at 07:00AM   \n",
       "\n",
       "              emotion  textlen  word_count  \\\n",
       "199832            joy       27           5   \n",
       "415133            joy      103          19   \n",
       "1041647       disgust      112          11   \n",
       "1307483           joy      101          19   \n",
       "1329653  anticipation      123          20   \n",
       "...               ...      ...         ...   \n",
       "1086237           joy      100          16   \n",
       "1069156  anticipation      112          22   \n",
       "362271            joy      101          19   \n",
       "117577        sadness       74           8   \n",
       "346903   anticipation      104          21   \n",
       "\n",
       "                                                                                              cleantext  \\\n",
       "199832                                                                                     wow how nice   \n",
       "415133                                             closed buy lots eurusd for pips total for today pips   \n",
       "1041647                                                                             does that even mean   \n",
       "1307483                                            closed buy lots eurusd for pips total for today pips   \n",
       "1329653  thank you very much for your service and for the follow fellow patriot isaluteyou unitedstates   \n",
       "...                                                                                                 ...   \n",
       "1086237                               faith hope and will last forever the greatest is love corinthians   \n",
       "1069156                                     the moments in your life are only once life september at pm   \n",
       "362271                              love like a river will cut a new path whenever it meets an obstacle   \n",
       "117577                                        watching the pagemaster skids thepagemaster bookishmovies   \n",
       "346903                           never give up on your dream me to make your dreams come november at am   \n",
       "\n",
       "         textlenafter  word_countafter             langs  \n",
       "199832             12                3  ('__label__en',)  \n",
       "415133             52               10  ('__label__en',)  \n",
       "1041647            19                4  ('__label__en',)  \n",
       "1307483            52               10  ('__label__en',)  \n",
       "1329653            94               15  ('__label__en',)  \n",
       "...               ...              ...               ...  \n",
       "1086237            65               11  ('__label__en',)  \n",
       "1069156            59               12  ('__label__en',)  \n",
       "362271             67               14  ('__label__en',)  \n",
       "117577             57                6  ('__label__en',)  \n",
       "346903             70               15  ('__label__en',)  \n",
       "\n",
       "[128379 rows x 10 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata[traindata.duplicated(['cleantext'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.drop_duplicates('cleantext', keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1346311, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save data in Pickle format to continue without doing the previous step againn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.to_pickle(\"traindataclean.pkl\")\n",
    "Testdata.to_pickle(\"Testdataclean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continue from save file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "traindata = pd.read_pickle(\"traindataclean.pkl\")\n",
    "Testdata = pd.read_pickle(\"Testdataclean.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let see the result from out cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words before: 1575264\n",
      "Number of unique words after: 528831\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "result = Counter(\" \".join(traindata['text'].values.tolist()).split(\" \")).items()\n",
    "print(\"Number of unique words before:\",len(result))\n",
    "result = Counter(\" \".join(traindata['cleantext'].values.tolist()).split(\" \")).items()\n",
    "print(\"Number of unique words after:\",len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.boxplot(traindata.textlenafter)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "plt.boxplot(traindata.word_countafter)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is many outlier so let see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata[traindata['word_countafter']>40]\n",
    "traindata=traindata.drop(traindata[(traindata.word_countafter>70)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the word frequency for every emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# conda install -c conda-forge wordcloud\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "joy_tweets = traindata[traindata.emotion == 'joy']\n",
    "joy_string = []\n",
    "for t in joy_tweets.cleantext:\n",
    "    joy_string.append(t)\n",
    "joy_string = pd.Series(joy_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(joy_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"joy words\")\n",
    "plt.show()\n",
    "\n",
    "disgust_tweets = traindata[traindata.emotion == 'disgust']\n",
    "disgust_string = []\n",
    "for t in disgust_tweets.cleantext:\n",
    "    disgust_string.append(t)\n",
    "disgust_string = pd.Series(disgust_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(disgust_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"disgust words\")\n",
    "plt.show()\n",
    "\n",
    "sadness_tweets = traindata[traindata.emotion == 'sadness']\n",
    "sadness_string = []\n",
    "for t in sadness_tweets.cleantext:\n",
    "    sadness_string.append(t)\n",
    "sadness_string = pd.Series(sadness_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(sadness_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"sadness words\")\n",
    "plt.show()\n",
    "\n",
    "trust_tweets = traindata[traindata.emotion == 'trust']\n",
    "trust_string = []\n",
    "for t in trust_tweets.cleantext:\n",
    "    trust_string.append(t)\n",
    "trust_string = pd.Series(trust_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(trust_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"trust words\")\n",
    "plt.show()\n",
    "\n",
    "anticipation_tweets = traindata[traindata.emotion == 'anticipation']\n",
    "anticipation_string = []\n",
    "for t in anticipation_tweets.cleantext:\n",
    "    anticipation_string.append(t)\n",
    "anticipation_string = pd.Series(anticipation_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(anticipation_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"anticipation words\")\n",
    "plt.show()\n",
    "\n",
    "surprise_tweets = traindata[traindata.emotion == 'surprise']\n",
    "surprise_string = []\n",
    "for t in surprise_tweets.cleantext:\n",
    "    surprise_string.append(t)\n",
    "surprise_string = pd.Series(surprise_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(surprise_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"surprise words\")\n",
    "plt.show()\n",
    "\n",
    "anger_tweets = traindata[traindata.emotion == 'anger']\n",
    "anger_string = []\n",
    "for t in anger_tweets.cleantext:\n",
    "    anger_string.append(t)\n",
    "anger_string = pd.Series(anger_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(anger_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"anger words\")\n",
    "plt.show()\n",
    "\n",
    "fear_tweets = traindata[traindata.emotion == 'fear']\n",
    "fear_string = []\n",
    "for t in fear_tweets.cleantext:\n",
    "    fear_string.append(t)\n",
    "fear_string = pd.Series(fear_string).str.cat(sep=' ')\n",
    "wordcloud = WordCloud(width=1600, height=800,max_font_size=200,collocations=False).generate(fear_string)\n",
    "plt.figure(figsize=(12,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"fear words\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop text that have less than 2 words and texlen morethan 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata=traindata.drop(traindata[(traindata.word_countafter<3)& (traindata.textlenafter > 50)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the distribution between each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = traindata['emotion'].unique()\n",
    "post_total = len(traindata)\n",
    "df1 = traindata['emotion'].value_counts()\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution in training sets')\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data is highly imbalance so i'll try to upsampling later to see if the model will perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I split the data for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = traindata.cleantext\n",
    "y = traindata.emotion\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see the distribution between train&test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = y_train.unique()\n",
    "post_total = len(y_train)\n",
    "df1 = y_train.value_counts()\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution in training sets')\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the histogram of the data\n",
    "labels = y_test.unique()\n",
    "post_total = len(y_test)\n",
    "df1 = y_test.value_counts()\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution  in testing sets')\n",
    "plt.grid(False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Bag of word (countvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "BOW_vectorizer = CountVectorizer(stop_words='english') \n",
    "BOW_vectorizer.fit(X_train)\n",
    "train_data_vectorizer_features=BOW_vectorizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Tfidfvertorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfidfvertorizer = TfidfVectorizer(stop_words = \"english\")\n",
    "train_data_Tfidf_features = Tfidfvertorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 nltktokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "word_tokenize = CountVectorizer(tokenizer = word_tokenize,stop_words='english')\n",
    "train_data_tweet_features = word_tokenize.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 other tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import (TreebankWordTokenizer,\n",
    "                           word_tokenize,\n",
    "                           wordpunct_tokenize,\n",
    "                           TweetTokenizer,\n",
    "                           MWETokenizer)\n",
    "wordpuncttokenize = CountVectorizer(tokenizer = wordpunct_tokenize,stop_words='english')\n",
    "train_data_wordpuncttokenize_features = wordpuncttokenize.fit_transform(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from tokenizers import (BertWordPieceTokenizer)\n",
    "bert_tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model&Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes (NB) is a common model for document classification. The main concept of Naive Bayes is to use the Bayes’ Theorem to estimate the joint probability of all the different words conditioned on each label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after trying every tokenizer I found that countvect with default tokenizer gave me the best resut so I use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "allemo = ['joy','sadness','trust','anticipation','disgust','surprise','anger','fear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_BOW = Pipeline([('vect', BOW_vectorizer),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_BOW.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb_BOW.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))\n",
    "#accuracy 0.5023104811149743"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRED= nb_BOW.predict(Testdata['cleantext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata['emotion']=PRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata[['id','emotion']].to_csv('NB_BOW.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb_tfidf = Pipeline([('vect', Tfidfvertorizer),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb_tfidf.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))\n",
    "#accuracy 0.4445335948411637"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb_word = Pipeline([('vect', word_tokenize),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_word.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb_word.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRED= nb_word.predict(Testdata['cleantext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata['emotion']=PRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata[['id','emotion']].to_csv('NB_WORD.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "nb_wordpuncttokenize = Pipeline([('vect', wordpuncttokenize),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))\n",
    "#accuracy 0.5023327687141541\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRED= nb_wordpuncttokenize.predict(Testdata['cleantext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata['emotion']=PRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata[['id','emotion']].to_csv('NB_wordpuncttokenize.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf'))])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_BOW = Pipeline([('vect', BOW_vectorizer),('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_BOW.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb_BOW.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))\n",
    "#accuracy 0.4445335948411637"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf'))])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_BOW = Pipeline([('vect', wordpuncttokenize),('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb_BOW.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = nb_BOW.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_wordpuncttokenize = Pipeline([('vect',  wordpuncttokenize),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=10, tol=None)),\n",
    "               ])\n",
    "sgd_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = sgd_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#CountVectorizer(stop_words='english')\n",
    "sgd_BOW = Pipeline([('vect', BOW_vectorizer),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=10, tol=None)),\n",
    "               ])\n",
    "sgd_BOW.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = sgd_BOW.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tfidfvertorizer\n",
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_TfidfTransformer = Pipeline([('vect', TfidfTransformer(smooth_idf=True,use_idf=True)),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=10, tol=None)),\n",
    "               ])\n",
    "sgd_TfidfTransformer.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = sgd_TfidfTransformer.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43merror\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#CountVectorizer(stop_words='english')\n",
    "logreg_BOW = Pipeline([('vect', BOW_vectorizer),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg_BOW.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg_BOW.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRED= logreg_BOW.predict(Testdata['cleantext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata['emotion']=PRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testdata[['id','emotion']].to_csv('LR_BOW.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_wordpuncttokenize = Pipeline([('vect', wordpuncttokenize),('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) \n",
    "#accuracy 0.5060473685774568\n",
    "#accuracy 0.4976820896852991 add ('tfidf', TfidfTransformer())  become worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_Tfidfvertorizer = Pipeline([('vect', Tfidfvertorizer),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg_Tfidfvertorizer.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg_Tfidfvertorizer.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word_tokenize\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_word_tokenize = Pipeline([('vect', word_tokenize),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg_word_tokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg_word_tokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf'))])\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_word_tokenize = Pipeline([('vect', word_tokenize), ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg_word_tokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = logreg_word_tokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I replace old keras to new onw(tensorflow keras)\n",
    "# \n",
    "#from keras.models import Model\n",
    "#from keras.layers import Input, Dense\n",
    "#from keras.layers import ReLU, Softmax\n",
    "\n",
    "import tensorflow as tf \n",
    "keras = tf.keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D, Embedding\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.layers import Reshape, Dropout, Dense,Multiply, Dot, Concatenate,Embedding,ReLU, Softmax\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input,Conv1D,MaxPooling1D,Dense,GlobalMaxPooling1D,Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500_word_tokenize = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500_word_tokenize.fit(traindata['cleantext'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500_word_tokenize.transform(traindata['cleantext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#wordpunct_tokenize\n",
    "BOW_500_wordpunct_tokenize = CountVectorizer(max_features=500, tokenizer=nltk.wordpunct_tokenize) \n",
    "\n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500_wordpunct_tokenize.fit(traindata['cleantext'])\n",
    "\n",
    "train_data_wordpunctfeatures_500 = BOW_500_wordpunct_tokenize.transform(traindata['cleantext'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'emotion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\env1\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\env1\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'emotion'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m traindata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m X_test \u001b[38;5;241m=\u001b[39m BOW_500_wordpunct_tokenize\u001b[38;5;241m.\u001b[39mtransform(Testdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleantext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[43mTestdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memotion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m## check dimension is a good habbit \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train.shape: \u001b[39m\u001b[38;5;124m'\u001b[39m, X_train\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\env1\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mD:\\ProgramData\\anaconda3\\envs\\env1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'emotion'"
     ]
    }
   ],
   "source": [
    "# standardize name (X, y) \n",
    "X_train = BOW_500_wordpunct_tokenize.transform(traindata['cleantext'])\n",
    "y_train = traindata['emotion']\n",
    "\n",
    "X_test = BOW_500_wordpunct_tokenize.transform(Testdata['cleantext'])\n",
    "y_test = Testdata['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata['emotion']=pred_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata[['id','emotion']].to_csv('CNN.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                      ('mnb', MultinomialNB(fit_prior=False)), \n",
    "                            ])\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(X_train, y_train)\n",
    "y_pred = text_mnb_stemmed.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/alokmalik/text-classification-using-svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(tree, n_estimators=100, max_samples=0.8,\n",
    "                        random_state=1)\n",
    "\n",
    "\n",
    "#Use pipeline to carry out steps in sequence with a single object\n",
    "#SVM's rbf kernel gives highest accuracy in this classification problem.\n",
    "\n",
    "#('vect', wordpuncttokenize)\n",
    "#RF_CountVectorizer = Pipeline([('vect', CountVectorizer(stop_words='english')), ('clf', RandomForestClassifier())])\n",
    "bag_wordpuncttokenize = Pipeline([('vect', wordpuncttokenize), ('clf', bag)])\n",
    "             \n",
    "bag_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "y_pred = bag_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED= bag_wordpuncttokenize.predict(Testdata['cleantext'])\n",
    "\n",
    "Testdata['emotion']=PRED\n",
    "\n",
    "#Testdata\n",
    "\n",
    "Testdata[['id','emotion']].to_csv('BaggingClassifier_wordpuncttokenize.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', SVC(kernel='rbf'))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/alokmalik/text-classification-using-svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "RF_wordpuncttokenize = Pipeline([('vect', wordpuncttokenize), ('clf', RandomForestClassifier())])\n",
    "\n",
    "             \n",
    "RF_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "y_pred = RF_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED= RF_wordpuncttokenize.predict(Testdata['cleantext'])\n",
    "\n",
    "Testdata['emotion']=PRED\n",
    "\n",
    "#Testdata\n",
    "\n",
    "Testdata[['id','emotion']].to_csv('RF_wordpuncttokenize.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/alokmalik/text-classification-using-svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "#Use pipeline to carry out steps in sequence with a single object\n",
    "#SVM's rbf kernel gives highest accuracy in this classification problem.\n",
    "\n",
    "#('vect', wordpuncttokenize)\n",
    "#RF_CountVectorizer = Pipeline([('vect', CountVectorizer(stop_words='english')), ('clf', RandomForestClassifier())])\n",
    "#\n",
    "\n",
    "RF_BOW_vectorizer = Pipeline([('vect', BOW_vectorizer), ('clf', RandomForestClassifier())])\n",
    "             \n",
    "RF_BOW_vectorizer.fit(X_train, y_train)\n",
    "\n",
    "y_pred = RF_BOW_vectorizer.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/alokmalik/text-classification-using-svm\n",
    "from sklearn.svm import SVC# Support Vector Machine\n",
    "\n",
    "#Use pipeline to carry out steps in sequence with a single object\n",
    "#SVM's rbf kernel gives highest accuracy in this classification problem.\n",
    "\n",
    "#('vect', wordpuncttokenize)\n",
    "svc_wordpuncttokenize = Pipeline([('vect', wordpuncttokenize), ('clf', SVC(kernel='rbf'))])\n",
    "            \n",
    "svc_wordpuncttokenize.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = svc_wordpuncttokenize.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED= svc_wordpuncttokenize.predict(Testdata['cleantext'])\n",
    "\n",
    "Testdata['emotion']=PRED\n",
    "\n",
    "#Testdata\n",
    "\n",
    "Testdata[['id','emotion']].to_csv('svc_wordpuncttokenize.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/alokmalik/text-classification-using-svm\n",
    "from sklearn.svm import SVC# Support Vector Machine\n",
    "\n",
    "#Use pipeline to carry out steps in sequence with a single object\n",
    "#SVM's rbf kernel gives highest accuracy in this classification problem.\n",
    "\n",
    "#('vect', wordpuncttokenize)\n",
    "svc_CountVectorizer = Pipeline([('vect', CountVectorizer(stop_words='english')), ('clf', SVC(kernel='rbf'))])\n",
    "            \n",
    "svc_CountVectorizer.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = svc_CountVectorizer.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=allemo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 FASTTEXT(Best score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format data & export to TXT because fasttext read data from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\"lr\": 0.01,\n",
    "                \"epoch\": 14,\n",
    "                \"wordNgrams\": 2,\n",
    "                \"dim\": 20}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['data'] = traindata['cleantext']+'__label__'+traindata['emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['data'].to_csv('fastnotokay1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainmodel&evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised('fastnotokay1.txt',**hyper_params)\n",
    "print(\"Model trained with the hyperparameter \\n {}\".format(hyper_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata['cleantext'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(traindata['cleantext'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate kaggle test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata['emotion'] = Testdata['cleantext'].apply(lambda x: model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata[['id','emotion']].to_csv('FAST22ngram.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Building the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Insights gained.\n",
    "\n",
    "- between these models I tried, I foud that Fasttext give me the best result on the competition with very fast training time \n",
    "- text from twitter are really messy so the text cleaning is very important (I go back many times to do the text cleaning)\n",
    "- this case BOW is better than Tfidf, may be the frequent word is really help us to classify but idf removed it.(at first I think TFIDF would perform better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Things I want to improve/try\n",
    "\n",
    "- other language models like bert(I've tried roberta but my computer keep dying & the estimated time train are 10 days ;(\n",
    "- try upsampling/other method to deal with the data imbalance.Because the majority of the data labels are 'joy,' so the model may be difficult to predict other emotions.\n",
    "- the spell check because text from social contain many typo (I've tried spellchecker/textblob but it took too long)\n",
    "- too many repeated words in tweet text I found when i plot a boxplot, I wantto remove them.\n",
    "- word2vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
